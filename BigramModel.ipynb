{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlS4sb0e/gT6IMefVddrlg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahuldewangan05/BuildingGPT/blob/main/BigramModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In this project we are making character based model which predicts the next character based on previous text"
      ],
      "metadata": {
        "id": "ysgfDnc-ZW9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using tine shakespere dataset where we have all the poems written by shakespere in text document format\n"
      ],
      "metadata": {
        "id": "ayt_oHxkXkkP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCvSejakXTBK",
        "outputId": "fc98ce8e-2670-4efa-a84a-df1666f86d19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-29 19:33:31--  https://raw.githubusercontent.com/Karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-03-29 19:33:32 (27.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Downloading tiny shakespere dataset\n",
        "!wget https://raw.githubusercontent.com/Karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## reading the text file\n",
        "with open('input.txt','r',encoding='utf-8') as f:\n",
        "  text=f.read()"
      ],
      "metadata": {
        "id": "LEg6ugRiYFXm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \",len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QjRpyK7YaHq",
        "outputId": "9aafe3fc-2fcb-4c20-e1ee-2697d3cd6c75"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## first 1000 characters\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0w13mCBqYkPL",
        "outputId": "7f96e300-b68d-45a9-c560-3adf992e45cd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## here are all the unique characters that occur in the text\n",
        "\n",
        "## text -> sequece of char , set -> find unique char, list -> make list\n",
        "chars=sorted(list(set(text)))\n",
        "\n",
        "vocab_size=len(chars)\n",
        "\n",
        "print(\"Unique character in this text are : \",''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abeeEoP0YvEK",
        "outputId": "459c4eed-4aa0-4a28-aba8-20c24c3ecf12"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique character in this text are :  \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create a mapping from character to integer"
      ],
      "metadata": {
        "id": "aClY6RyjsGyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## stoi is dictnary having elements as character:index\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "\n",
        "## itos is dictnary having elements as index:character\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "## function to convert every character in string to it desired index acc to stoi and returns list\n",
        "encode = lambda s: [stoi[i] for i in s]\n",
        "\n",
        "## function to convert every index in list to it desired character and return a string\n",
        "decode = lambda s: ''.join([itos[index] for index in s])\n",
        "\n",
        "print(encode(\"hello world\"))\n",
        "print(decode(encode(\"hello world\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bln7zxfFaNF-",
        "outputId": "6004547a-823d-4ac5-9070-4d28876a1287"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
            "hello world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encode the entire text of shakespeare poem and store it into torch.tensor"
      ],
      "metadata": {
        "id": "nphOylq4z8Ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data=torch.tensor(encode(text),dtype=torch.long)\n",
        "print(\"length of data (encoded text file) :\",len(data))\n",
        "print(\"Shape of data (encoded text file) : \",data.shape)\n",
        "print(\"dtatype of data (encoded text file) :\",data.dtype)\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nm4mnnWGu01w",
        "outputId": "c67e74e6-8981-472e-8e51-cbd1cb027bcc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of data (encoded text file) : 1115394\n",
            "Shape of data (encoded text file) :  torch.Size([1115394])\n",
            "dtatype of data (encoded text file) : torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## split the data into traning and testing\n",
        "\n",
        "Note : we can't use train test split library because we have sequential data where sequence matters"
      ],
      "metadata": {
        "id": "5yX_h6Rv2pJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## taking first 90% data for training and rest data for testing\n",
        "n=int(0.9*len(data))\n",
        "train_data=data[:n]\n",
        "test_data=data[n:]"
      ],
      "metadata": {
        "id": "m3lhOB4j0wvX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## defining block size to send to model for training\n",
        "block_size=8\n",
        "\n",
        "## defining sample training and testing data with their context and target integer\n",
        "x=train_data[:block_size]\n",
        "y=train_data[1:block_size+1]\n",
        "\n",
        "print(\"sample training data \",x)\n",
        "print(\"sample testing data \",y)\n",
        "\n",
        "## printing how the dataset will look like\n",
        "for t in range(block_size):\n",
        "  context= x[:t+1]\n",
        "  target= y[t]\n",
        "  print(\"when context is \",context,\" the target is \",target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjoiFble3PQe",
        "outputId": "09f7b2fa-c596-40e2-e6ae-0f82bab31aa7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample training data  tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
            "sample testing data  tensor([47, 56, 57, 58,  1, 15, 47, 58])\n",
            "when context is  tensor([18])  the target is  tensor(47)\n",
            "when context is  tensor([18, 47])  the target is  tensor(56)\n",
            "when context is  tensor([18, 47, 56])  the target is  tensor(57)\n",
            "when context is  tensor([18, 47, 56, 57])  the target is  tensor(58)\n",
            "when context is  tensor([18, 47, 56, 57, 58])  the target is  tensor(1)\n",
            "when context is  tensor([18, 47, 56, 57, 58,  1])  the target is  tensor(15)\n",
            "when context is  tensor([18, 47, 56, 57, 58,  1, 15])  the target is  tensor(47)\n",
            "when context is  tensor([18, 47, 56, 57, 58,  1, 15, 47])  the target is  tensor(58)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Macking batches of input data"
      ],
      "metadata": {
        "id": "NbJtoRzDMyT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## setting seed so that random number so generated is same\n",
        "torch.manual_seed(1337)\n",
        "batch_size=4 ## we try to send 4 batches to model for training\n",
        "block_size=8 ## define the context length\n",
        "\n",
        "## function to genrate a small batch of data of inputs x and target y\n",
        "def get_batch(split):\n",
        "  data = train_data if split==\"train\" else test_data\n",
        "  ix=torch.randint(len(data)-block_size,(batch_size,))\n",
        "  x=torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y=torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x,y\n",
        "\n",
        "xb,yb=get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('target')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print(\"-------\")\n",
        "\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context=xb[b,:t+1]    ## since in xb is stack so if would be list it would be similar to xb[b[:t+1]]\n",
        "    target=yb[b,t]\n",
        "    print(\"when input is \",context,\" the target is \",target)"
      ],
      "metadata": {
        "id": "X_nKRTqD5qNl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05064d2d-cbc3-4f8b-ab49-73f14df605e1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "target\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "-------\n",
            "when input is  tensor([24])  the target is  tensor(43)\n",
            "when input is  tensor([24, 43])  the target is  tensor(58)\n",
            "when input is  tensor([24, 43, 58])  the target is  tensor(5)\n",
            "when input is  tensor([24, 43, 58,  5])  the target is  tensor(57)\n",
            "when input is  tensor([24, 43, 58,  5, 57])  the target is  tensor(1)\n",
            "when input is  tensor([24, 43, 58,  5, 57,  1])  the target is  tensor(46)\n",
            "when input is  tensor([24, 43, 58,  5, 57,  1, 46])  the target is  tensor(43)\n",
            "when input is  tensor([24, 43, 58,  5, 57,  1, 46, 43])  the target is  tensor(39)\n",
            "when input is  tensor([44])  the target is  tensor(53)\n",
            "when input is  tensor([44, 53])  the target is  tensor(56)\n",
            "when input is  tensor([44, 53, 56])  the target is  tensor(1)\n",
            "when input is  tensor([44, 53, 56,  1])  the target is  tensor(58)\n",
            "when input is  tensor([44, 53, 56,  1, 58])  the target is  tensor(46)\n",
            "when input is  tensor([44, 53, 56,  1, 58, 46])  the target is  tensor(39)\n",
            "when input is  tensor([44, 53, 56,  1, 58, 46, 39])  the target is  tensor(58)\n",
            "when input is  tensor([44, 53, 56,  1, 58, 46, 39, 58])  the target is  tensor(1)\n",
            "when input is  tensor([52])  the target is  tensor(58)\n",
            "when input is  tensor([52, 58])  the target is  tensor(1)\n",
            "when input is  tensor([52, 58,  1])  the target is  tensor(58)\n",
            "when input is  tensor([52, 58,  1, 58])  the target is  tensor(46)\n",
            "when input is  tensor([52, 58,  1, 58, 46])  the target is  tensor(39)\n",
            "when input is  tensor([52, 58,  1, 58, 46, 39])  the target is  tensor(58)\n",
            "when input is  tensor([52, 58,  1, 58, 46, 39, 58])  the target is  tensor(1)\n",
            "when input is  tensor([52, 58,  1, 58, 46, 39, 58,  1])  the target is  tensor(46)\n",
            "when input is  tensor([25])  the target is  tensor(17)\n",
            "when input is  tensor([25, 17])  the target is  tensor(27)\n",
            "when input is  tensor([25, 17, 27])  the target is  tensor(10)\n",
            "when input is  tensor([25, 17, 27, 10])  the target is  tensor(0)\n",
            "when input is  tensor([25, 17, 27, 10,  0])  the target is  tensor(21)\n",
            "when input is  tensor([25, 17, 27, 10,  0, 21])  the target is  tensor(1)\n",
            "when input is  tensor([25, 17, 27, 10,  0, 21,  1])  the target is  tensor(54)\n",
            "when input is  tensor([25, 17, 27, 10,  0, 21,  1, 54])  the target is  tensor(39)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using bigram language model"
      ],
      "metadata": {
        "id": "HcF6rbMZs0Bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_MtKUU0Osw1",
        "outputId": "c5a2e4f9-f6ed-4737-ca99-f79e30bdb950"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table=nn.Embedding(vocab_size,vocab_size)\n",
        "\n",
        "  def forward(self,idx,targets=None):\n",
        "    ## idx and target are both (B,T) tensor of integrs\n",
        "    logists=self.token_embedding_table(idx) ## (B,T,C)\n",
        "\n",
        "    if targets is None:\n",
        "      loss=None\n",
        "    else:\n",
        "      B,T,C =logists.shape\n",
        "      logists=logists.view(B*T,C)\n",
        "      targets=targets.view(-1)\n",
        "      loss=F.cross_entropy(logists,targets)\n",
        "\n",
        "    return logists, loss\n",
        "\n",
        "  def generate(self,idx,max_new_token):\n",
        "    ## idx is (B,T) array of indices in the current context\n",
        "    for _ in range(max_new_token):\n",
        "      ## get the predictions\n",
        "      logists, loss=self(idx)\n",
        "      ## focus only on the last time step\n",
        "      logists = logists[:,-1,:] ## becomes (B,C)\n",
        "      probs=F.softmax(logists, dim=1) #(B,C)\n",
        "      # sample from the distribution\n",
        "      idx_next=torch.multinomial(probs,num_samples=1) # (B,1)\n",
        "      ## append sampled index to the running sequence\n",
        "      idx=torch.cat((idx,idx_next),dim=1) # (B,T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logists,loss= m.forward(xb,yb)\n",
        "print(logists.shape)\n",
        "print(loss)\n",
        "\n",
        "idx=torch.zeros((1,1),dtype=torch.long)\n",
        "print(decode(m.generate(idx=torch.zeros((1,1),dtype=torch.long),max_new_token=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjV6YwOAV9bg",
        "outputId": "66af0ba0-1b41-43e9-b5cc-0ced7ad1ab4b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Brief explaination about terms B,T,C\n",
        "B -> batch size (here 32)\n",
        "\n",
        "T -> time step or block size (here 8)\n",
        "\n",
        "C -> channel or vocab size or size of embedding vector (here 65)\n",
        "\n",
        "Note : here in forward fucntion of BigramModel the logits and target are flattend because we are using cross validation frunction for calculating loss so this cross validation function expect :\n",
        "\n",
        "1. logits (input) as a 2d vector (N,C) where N->tokens and C-> no. of classes(here vocab size)\n",
        "so we flatten logits from shape (B,T,C) to (B*T,C)\n",
        "\n",
        "2. target as 1d vector (N) were N->token\n",
        "so we flatten target from shape (B,T) to (B*T)\n",
        "\n",
        "here B*T represent all the token in single line since B(Batch_Size) * T(block_size)"
      ],
      "metadata": {
        "id": "RC0JToWaXVtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDhMJvnYWknn",
        "outputId": "dddc88cd-1cfe-4181-c25a-a7ca1b82ccf1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
              "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
              "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
              "        [25, 17, 27, 10,  0, 21,  1, 54]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joyBzyD0WlyX",
        "outputId": "ff811e47-f7c3-482f-8018-8bc694543b42"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
              "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
              "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
              "        [17, 27, 10,  0, 21,  1, 54, 39]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Using AdmaW optimizer in pytorch here m.parameter()-> passes all the trainable parameter to optimiser AND lr -> learning rate\n",
        "optimizer=torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "hUPQmdRpvf8b"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## note : the more we run this codeblock more the loss will be minimised\n",
        "\n",
        "## minimising the loss\n",
        "batch_size=32\n",
        "\n",
        "## range is similar to epoch in tensorflow\n",
        "for steps in range(1000):\n",
        "\n",
        "  # sample a batch of data\n",
        "  xb,yb=get_batch('train')\n",
        "\n",
        "  #evaluate the loss by passing x_train and y_train to model\n",
        "  logists,loss=m(xb,yb)\n",
        "  # zero out the gradient\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  # compute gradient\n",
        "  loss.backward()\n",
        "  # update model parameter\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "id": "A1Mg85NtwIsT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "946b5fce-beef-45f9-a419-0048feb7a188"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.7218432426452637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## now after minimising the loss if again try model to print next character for the character embedded as zero -> it gives better result\n",
        "print(decode(m.generate(idx=torch.zeros((1,1),dtype=torch.long),max_new_token=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qfpQq1_aAqX",
        "outputId": "286624a3-0d16-413f-9e12-8f623d23f847"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "olylvLLko'TMyatyIoconxad.?-tNSqYPsx&bF.oiR;BD$dZBMZv'K f bRSmIKptRPly:AUC&$zLK,qUEy&Ay;ZxjKVhmrdagC-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note  : print(decode(m.generate(idx=torch.zeros((1,1),dtype=torch.long),max_new_token=100)[0].tolist()))\n",
        "\n",
        "this line means :\n",
        " - Starting with a blank/zero token\n",
        " - Asking the model \"what character should come next?\" 100 times\n",
        " - Converting the resulting sequence of token IDs back to readable text"
      ],
      "metadata": {
        "id": "l7t0I2R8bkIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Note**: till here we have build a very simple model which predicts what comes next based on the last character of context being provided to model"
      ],
      "metadata": {
        "id": "7wmPErfDcI5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self attention blocks for procecessing tokens\n"
      ],
      "metadata": {
        "id": "FMKu4NCDKQZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## consider the following toy example\n",
        "\n",
        "## setting seed to get same result everytime\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 ## batch, time_step, channel\n",
        "x=torch.randn(B,T,C) ## random initialisation of tensor x with shape(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFITVU13amt0",
        "outputId": "7177e549-1c2b-4c34-fe31-92bdb2489b0d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## note : here the concept we are using to getnext character is\n",
        "\n",
        "for predicting the next character we just take average of all the charater appearing before the current character to predict the next character"
      ],
      "metadata": {
        "id": "64E2N4FeZX_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## initialising a tensor named xbow filled dwith zeros of shape (B,T,C)\n",
        "xbow=torch.zeros((B,T,C))\n",
        "\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    ## collecting the previous token from current token\n",
        "    xprev=x[b,:t+1] ## (t,C)\n",
        "    ## adding the average till the current term in xbow\n",
        "    xbow[b,t]=torch.mean(xprev,0)"
      ],
      "metadata": {
        "id": "1_jxiKkRZQXW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rahuldewangan05/BuildingGPT.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlGt_XSNkeEz",
        "outputId": "fbfa8b84-7070-4054-fed2-4beaa5e2b720"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'BuildingGPT'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (3/3), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"dewanganr28@gmail.com\"\n",
        "!git config --global user.name \"rahuldewangan05\""
      ],
      "metadata": {
        "id": "Dg8U27S1kw89"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7NaRteCplD5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS9eYce_k7kV",
        "outputId": "2f3a9f9a-1250-40f0-d114-f1b42a790553"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "warning: adding embedded git repository: BuildingGPT\n",
            "\u001b[33mhint: You've added another git repository inside your current repository.\u001b[m\n",
            "\u001b[33mhint: Clones of the outer repository will not contain the contents of\u001b[m\n",
            "\u001b[33mhint: the embedded repository and will not know how to obtain it.\u001b[m\n",
            "\u001b[33mhint: If you meant to add a submodule, use:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit submodule add <url> BuildingGPT\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: If you added this path by mistake, you can remove it from the\u001b[m\n",
            "\u001b[33mhint: index with:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit rm --cached BuildingGPT\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: See \"git help submodule\" for more information.\u001b[m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"BigramModel\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CsqbtHTk9un",
        "outputId": "50af3c98-b0d4-4b06-81b3-b3c176170a7a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main 6ec8d48] BigramModel\n",
            " 1 file changed, 1 insertion(+)\n",
            " create mode 160000 BuildingGPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNNm363llLPI",
        "outputId": "62a5d97a-3655-4795-a8c0-d66a3c8a5cfb"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hnxXkeRslNNc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}